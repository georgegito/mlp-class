{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import Input\n",
    "import utils\n",
    "import metrics\n",
    "\n",
    "# MNIST dataset params\n",
    "num_classes = 10 # 0-9 digits\n",
    "num_features = 784 # img shape: 28*28\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# pre-process data\n",
    "X_train, y_train, X_test, y_test = utils.preprocess(X_train, y_train, X_test, y_test, num_classes, num_features, print_summary=False)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert target classes to categorical ones (one-hot encoding) - only for training and tuning\n",
    "y_train_enc = to_categorical(y_train, num_classes)\n",
    "y_test_enc = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network loss function\n",
    "loss =tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# network metrics\n",
    "eval_metrics = [metrics.f1]\n",
    "\n",
    "# training batch size\n",
    "batch_size = 256\n",
    "\n",
    "# training epochs\n",
    "epochs = 1000\n",
    "\n",
    "# print options during training\n",
    "verbose = 1\n",
    "\n",
    "# early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **$1$. Fine Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    # hyperparams\n",
    "    n_hidden_1      =   hp.Choice(\"units_1\", [64, 128])\n",
    "    n_hidden_2      =   hp.Choice(\"units_2\", [256, 512])\n",
    "    a_reg           =   hp.Choice(\"a_reg\", [0.1, 0.001, 0.000001])\n",
    "    learning_rate   =   hp.Choice(\"learning_rate\", [0.1, 0.01, 0.001])\n",
    "\n",
    "    # mlp model\n",
    "    mlp_rmsprop_ft = keras.Sequential(name=\"MLP_RMSProp_FT\")\n",
    "\n",
    "    mlp_rmsprop_ft.add(Input(shape=(num_features,)))\n",
    "\n",
    "    mlp_rmsprop_ft.add(keras.layers.Dense(name=\"hidden_layer_1\", units=n_hidden_1, activation=\"relu\", \n",
    "                        kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "\n",
    "    mlp_rmsprop_ft.add(keras.layers.Dense(name=\"hidden_layer_2\", units=n_hidden_2, activation=\"relu\", \n",
    "                        kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "\n",
    "    mlp_rmsprop_ft.add(keras.layers.Dense(name=\"output_layer\", units=num_classes, activation=\"softmax\", \n",
    "                        kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "    \n",
    "    mlp_rmsprop_ft.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "                            loss=loss, \n",
    "                            metrics=eval_metrics)\n",
    "\n",
    "    return mlp_rmsprop_ft\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=20,\n",
    "    overwrite=True,\n",
    "    directory=\"tuning\",\n",
    "    project_name=\"mlp_tuning\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train_enc, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test_enc), verbose=verbose, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_model.summary()\n",
    "print(best_hyperparameters.get(\"units_1\"))\n",
    "print(best_hyperparameters.get(\"units_2\"))\n",
    "print(best_hyperparameters.get(\"a_reg\"))\n",
    "print(best_hyperparameters.get(\"learning_rate\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **$2$. Optimal Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $a$. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get params from tuning - currently using some random values\n",
    "n_hidden_1      =   128\n",
    "n_hidden_2      =   512\n",
    "a_reg           =   0.001\n",
    "learning_rate   =   0.001\n",
    "\n",
    "# mlp model\n",
    "mlp_rmsprop_opt = keras.Sequential(name=\"MLP_RMSProp_Opt\")\n",
    "\n",
    "mlp_rmsprop_opt.add(Input(shape=(num_features,)))\n",
    "\n",
    "mlp_rmsprop_opt.add(keras.layers.Dense(name=\"hidden_layer_1\", units=n_hidden_1, activation=\"relu\", \n",
    "                    kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "\n",
    "mlp_rmsprop_opt.add(keras.layers.Dense(name=\"hidden_layer_2\", units=n_hidden_2, activation=\"relu\", \n",
    "                    kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "\n",
    "mlp_rmsprop_opt.add(keras.layers.Dense(name=\"output_layer\", units=num_classes, activation=\"softmax\", \n",
    "                    kernel_regularizer=regularizers.l2(a_reg), kernel_initializer=initializers.HeNormal()))\n",
    "\n",
    "mlp_rmsprop_opt.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "                        loss=loss, \n",
    "                        metrics=eval_metrics)\n",
    "\n",
    "history = mlp_rmsprop_opt.fit(X_train, y_train_enc, epochs=100, validation_data=(X_test, y_test_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $b$. Results - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $i)$ Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classes\n",
    "y_pred = utils.predict_classes(mlp_rmsprop_opt, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "confusion_matrix_ = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(confusion_matrix_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "multilabel_confusion_matrix_ = multilabel_confusion_matrix(y_test, y_pred)\n",
    "confusion_matrix_ = np.sum(multilabel_confusion_matrix_, axis = 0)\n",
    "ConfusionMatrixDisplay(confusion_matrix_).plot()\n",
    "\n",
    "tn = confusion_matrix_[0][0]\n",
    "fp = confusion_matrix_[0][1]\n",
    "fn = confusion_matrix_[1][0]\n",
    "tp = confusion_matrix_[1][1]\n",
    "sum = tn + fp + fn + tp\n",
    "\n",
    "print(\"TN =\", tn, \"\\nFP =\", fp, \"\\nFN =\", fn, \"\\nTP =\", tp, \"\\nSUM =\", sum)\n",
    "\n",
    "accuracy = (tp + tn) / sum\n",
    "recall = tp / (tp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Accuracy =\", accuracy, \"\\nRecall =\", recall, \"\\nPrecision =\", precision, \"\\nF1 =\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ii)$ Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_train = history.history[\"loss\"]\n",
    "loss_test = history.history[\"val_loss\"]\n",
    "\n",
    "f1_train = history.history[\"f1\"]\n",
    "f1_test = history.history[\"val_f1\"]\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "plt.plot(loss_train, label=\"Training\")\n",
    "plt.plot(loss_test, label=\"Testing\")\n",
    "plt.title(mlp_rmsprop_opt.name + \" - Loss curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/\" + mlp_rmsprop_opt.name + \"_Loss.jpg\", dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(f1_train, label=\"Training\")\n",
    "plt.plot(f1_test, label=\"Testing\")\n",
    "plt.title(mlp_rmsprop_opt.name + \" - F1 curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"fig/\" + mlp_rmsprop_opt.name + \"_F1.jpg\", dpi=1200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
